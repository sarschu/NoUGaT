#!/usr/bin/env python
# encoding: utf-8
"""
PREPROCESSING
INPUT: line (@janthans @SvenOrnelis BBQ? Wie, wat , waar? :-D #aanwezig) 
OUTPUT: tokenized sentence with replacements (™ ™ BBQ? Wie, wat , waar? • aanwezig)

Created by Bart Desmet on 2013-02-04.
Adapated by Orphee De Clercq on 2013-04-29.
Copyright (c) 2013 LT3. All rights reserved.

Placeholders list
smileys:    "•" (all smileys + tags that have no closing tag)
tags:       "±" (typography, photo, quotes)
hyperlinks: "∞"
NE:         "™" (NER + atreplies without @)    
"""

import os
import re
import codecs
import random
import sys
import subprocess
import util
import logging
import socket
v = False

gazetteer_file = util.STATIC_DIR+'/NE/gazetteer.clean'
celex_file = util.STATIC_DIR+'/NE/celex.clean'
training_file = util.STATIC_DIR+'/NE/trainingNER.txt'
make_features = util.STATIC_DIR+'/NE/makeNEFeatures.py'
rewrite_log = logging.getLogger("norm.rewrite")
logfile = util.logfile

def correct_punctuation_flooding(t):
    # Lowercase the string, to find flooding with case alternation
    lower_t = t.lower()
    
    # Reduce character repetitions to max 2, except for numbers
    corrected_t = re.sub(r"([.?!:;,])(\1{2,})", r"\1\1", lower_t)
    
    # Replace 2 dots by 3 dots
    corrected_t = corrected_t.replace("..", "...")
    if v: print repr(corrected_t)
    
    # If correction was necessary, return the lowercased corrected string.
    # Else, return the original string with capitalization
    if lower_t != corrected_t:
        return corrected_t
    else: return t

def correct_flooding(t):
    # Lowercase the string, to find flooding with case alternation
    lower_t = t.lower()
    
    # Reduce character repetitions to max 2, except for numbers
    corrected_t = re.sub(ur"([^0-9±™•])(\1{2,})", r"\1\1", lower_t)
    # if v: print repr(corrected_s)
    
    # Reduce repetitions of substrings to max 2
    corrected_t = re.sub(ur"([^0-9±™•]{2,})(\1{2,})", r"\1\1", corrected_t)
    
    # Replace 2 dots by 3 dots
    corrected_t = corrected_t.replace("..", "...")
    if v: print repr(corrected_t)
    
    # If correction was necessary, return the lowercased corrected string.
    # Else, return the original string with capitalization
    if lower_t != corrected_t:
        return corrected_t
    else: return t

def replace_smileys(t):
    t = re.sub(r"B-?(\)\)?)", u"•", t)
    t = re.sub(r"<+/*3+", u"•", t)
    t = re.sub(r"[O>}\])]?[;:8=][',]?[-~]?([sSdDpPcCoO#@*$|]|\)\)?|\(\(?)=?", u"•", t)
    t = re.sub(r"([^ ])±", r"\1 •", t)
    if v: print repr(t)
    
    return t

def replace_smileytags(t):
    new_t = []
    for element in t.split(" "):
        p = re.compile(r"\[[a-z=]+\]")
        if p.match(element):
            split = element.split('[')
            close = "[/" + split[1]
            e = re.compile(r"=")
            if e.match(element):
                split2 = split[1].split('=')
                close = "[/" + split2[0] + "]"
            if close in t:
                element = element
            else: element = re.sub(r"\[[^\[\]]+\]", u"•", element)
            new_t.append(element)
        else: new_t.append(element)
    t = " ".join(new_t)
    if v: print repr(t)
    
    return t

def replace_tags(t):
    t = re.sub(r"\[[^\[\]]+\]", u"±", t)
    if v: print repr(t)
    
    return t

def replace_atreplies(t):
    t = re.sub(r"@[\w\d]+", u"™", t)
    if v: print repr(t)
    
    return t

def replace_hyperlinks(t):
    t = re.sub(r"http:/\S+", u"∞", t)
    t = re.sub(r"www\.\S+", u"∞", t)
    if v: print repr(t)
    
    return t
    
def replace_ne(t,norm):
    pos,tok = run_texsis(t)
    t = run_ner(t,pos,tok,norm)
    
    return t

def remove_hashtags(t):
    t = re.sub(r"#([\w\d]+)", r"\1", t)
    
    return t

def replace_cgn_anno(t):
    # print t
    if t == "TextGrid" or re.match("[A-Z]\d",t): # Document separators
        t = ""
    # t = re.sub(ur"ggg", ur"", t) # Remove laughter markers
    t = re.sub(ur"[Xx]xx", ur"™", t) # Incomprehensible NEs
    #t = re.sub(ur"Xxx", ur"™", t)
    t = re.sub(ur"[^\s]*[A-Z]+[^\s]*", ur"™", t) # All capitalized words are NEs
    for el in ['aha', 'ah', 'ai', 'au', 'bah', 'boe', 'bwa', 'eih', 'eikes',
    'goh', 'haha', 'ha', 'hei', 'ho', 'hum hum', 'hum', 'o jee', 'jee', 'mm-hu',
    'mmm', 'oeh', 'oei', 'oesje', 'oho', 'oh', 'o', 'poeh', 'pst', 'sjt', 'sst',
    'tut tut', 'tut', 'uh', 'uhm', 'uhu', 'wauw', 'woh', 'zuh', 'zulle', 'ggg']:
        # Remove prosody and laughter markers
        t = re.sub(ur"^%s[ ?.]+$" % el, "", t)
        t = re.sub(ur"^%s " % el, "", t)
        t = re.sub(ur" %s " % el, " ", t)
        t = re.sub(ur" %s$" % el, "", t)
        t = re.sub(ur" %s([ ?.]+)" % el, r" \1", t)
    for el in [u'hé', u'hè']:
        t = re.sub(ur"^%s[ ?.]+$" % el, "", t)
        t = re.sub(ur"^%s " % el, "he ", t)
        t = re.sub(ur" %s " % el, " he ", t)
        t = re.sub(ur" %s$" % el, " he", t)
        t = re.sub(ur" %s([ ?.]+)" % el, r" he\1", t)
    t = re.sub(ur"(([^0-9±™•\s]{1,3})\s)\1+(\2[^\s]+)", r"\3", t)
    
    # Do basic tokenisation
    t = re.sub(ur"...", " ...", t)
    t = re.sub(ur".", " .", t)
    t = re.sub(ur"\?", " ?", t)
    t = re.sub(ur"  ", " ", t)
    
    return t

def remove_linebreaks(t):
    return t.replace("<LINEBREAK>", "")
    
def run_texsis(t):
    rewrite_log.info("texsis started")
    log = open(logfile,"a")
    texsis_dir = util.get_random_tmp_path()
    os.system('mkdir '+texsis_dir)
    texsis_file = texsis_dir+"/texsis_file.sbd"
    f = codecs.open(texsis_file,'w','utf8')
    f.write(t)
    f.close()
    cmd = u'tok %s nl' %texsis_dir
    cmd2 = u'pos %s nl' %texsis_dir
    rewrite_log.info("texsis finished")
    tex1 = subprocess.Popen(["tok",texsis_dir,"nl"],stdout=log,stderr=log,shell=False)
    tex1.wait()#os.system(cmd)

    #print out
    #out,err = tex1.communicate()

    tex2 = subprocess.Popen(["pos",texsis_dir,"nl"],stdout=log,stderr=log,shell=False)
    tex2.wait()
    log.close()
    #out,err = tex2.communicate()
    #logging.debug(out)
    #os.system(cmd2)
    rewrite_log.info("texsis ran")
    posname = "texsis_file.pos"
    tokname = "texsis_file.tok"
    tok_file = texsis_dir+"/"+tokname
    pos_file = texsis_dir+"/"+posname
    log.close()
    return pos_file, tok_file

def get_ne_tags(ne):
    return [x.strip().split(" ")[-1] for x in codecs.open(ne,'r','utf8') if x.strip()]
    
def get_tokens(tok):
    return [x.strip().split(" ")[0] for x in codecs.open(tok,'r','utf8') if x.strip()]

def run_ner(t,pos_file,tok_file,norm):    
    rewrite_log.info("run NER")
    
    #feature_file = util.get_random_tmp_path()
    process = subprocess.Popen(['python',make_features,'nl',pos_file,tok_file,gazetteer_file,celex_file],stdout= subprocess.PIPE,stderr= subprocess.PIPE,shell=False)
    process.wait()
    #featuref = codecs.open(feature_file,'w','utf8')
    out,err = process.communicate()
    out = out.decode('utf8')
    rewrite_log.debug(out)
    rewrite_log.debug(err)
    #featuref.write(out)
    #featuref.close()
    
    #features = codecs.open(feature_file,'r','utf8')
    feature_lines = out.split("\n") 
    
    tokens = get_tokens(tok_file)

    index = 0
    for line in feature_lines:
        norm.s_timbl.send(("c "+line+"\n").encode("utf-8"))
        data = norm.s_timbl.recv(256)     

                                                         
        tag = re.sub("CATEGORY {"," ",data)
        tag = re.sub("}\n"," ",tag )

        if tag == "NE":
            tokens[index] = u"™"
        index +=1
  
    #rm all tmp files that have been built
    t = " ".join(tokens)
    dir = tok_file.split("/")
    dir_rm = "/".join(dir[:len(dir)-1])
    #os.system("rm "+feature_file)
    #remove all tempfiles and dirs
    cmd3 = 'rm -r '+dir_rm
    os.system(cmd3)
    rewrite_log.info("NER finished")
    return unicode(t)

def rewrite_text(t, norm, rewrite_type=None):
    rewrite_log.info("rewrite text")
    t = t.strip()
    if rewrite_type == "sms":
        t = replace_hyperlinks(t)
        t = replace_smileys(correct_flooding(correct_punctuation_flooding(t)))
        # t schrijven naar tempfile en NER uitvoeren.
        t = replace_ne(t,norm)
        t = smallcaps_text(t)
        t = tokenize_placeholders(t)
    elif rewrite_type == "sns":
        t = remove_linebreaks(t)
        t = replace_tags(replace_smileytags(replace_hyperlinks(t)))
        t = replace_smileys(correct_flooding(correct_punctuation_flooding(t)))
        t = replace_ne(t,norm)
        t = smallcaps_text(t)
        t = tokenize_placeholders(t)
    elif rewrite_type == "twe":
        t = remove_hashtags(t)
        t = replace_atreplies(t)
        t = replace_smileys(correct_flooding(correct_punctuation_flooding(t)))
        t = replace_ne(t,norm)
        t = smallcaps_text(t)
        t = tokenize_placeholders(t)
    elif rewrite_type =="cgn":
        t = replace_cgn_anno(t)
        t = replace_smileys(correct_flooding(correct_punctuation_flooding(t)))
        t = smallcaps_text(t)
        t = tokenize_placeholders(t)
    else:
        print "Specify rewrite type"
        raise
    rewrite_log.info("text rewritten")
    # print t
    return t

    
def tokenize_text():
    pass
    
def smallcaps_text(t):
    lower_t = t.lower()
    return lower_t
    
def tokenize_placeholders(t):
     #print t.encode('utf8')
    before_t = re.sub(ur"([^ ])([•±∞™])", ur"\1 \2", t)
    #print before_t.encode('utf8')
    after_t  = re.sub(ur"([•±∞™])([^ ])", ur"\1 \2", before_t)
    #print after_t.encode('utf8')
    return after_t

def main():
     pass
            
if __name__ == '__main__':
    main()
